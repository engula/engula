// Copyright 2022 The Engula Authors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
// http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// This file contains data structures that only node will use.

syntax = "proto3";

package serverpb.v1;

import "eraft.proto";
import "engula/server/v1/metadata.proto";

message SnapshotMeta {
  EntryID apply_state = 1;
  engula.server.v1.GroupDesc group_desc = 2;
  repeated SnapshotFile files = 3;
}

message SnapshotFile {
  // The relative path of snapshot file. eg `DATA/1.sst`, `META`.
  bytes name = 1;
  uint32 crc32 = 2;
  uint64 size = 3;
}

/// A NodeIdent uniquely identifies a node in the cluster.
message NodeIdent {
  bytes cluster_id = 1;
  uint64 node_id = 2;
}

enum ReplicaLocalState {
  /// With membership, but couldn't supply service.  It is used in group
  /// creation.
  INITIAL = 0;
  /// Without membership, only response raft messages.
  PENDING = 1;
  NORMAL = 2;
  /// The service and memory states are shutdown and cleans, but disk data still
  /// exists.
  TERMINATED = 3;
  TOMBSTONE = 4;
}

message ReplicaMeta {
  uint64 group_id = 1;
  uint64 replica_id = 2;
  ReplicaLocalState state = 3;
}

message EntryID {
  uint64 index = 1;
  uint64 term = 2;
}

message RaftLocalState {
  uint64 replica_id = 1;
  EntryID last_truncated = 3;
}

/// For dest group:
///   INITIAL -> MIGRATING -> MIGRATED -> FINISHED
///           -> ABORT
///
/// For source group:
///   MIGRATING -> MIGRATED -> FINISHED
///
enum MigrationStep {
  /// The dest group shold send request to source group to begin migration. Once
  /// the source group agrees to start the migration, the migration process must
  /// be completed.
  PREPARE = 0;
  MIGRATING = 1;
  /// Used in dest group, it should notify the source group that migration has
  /// already finished.
  MIGRATED = 2;

  FINISHED = 3;
  ABORTED = 4;
}

message MigrationState {
  /// The descriptor of migration.
  engula.server.v1.MigrationDesc migration_desc = 1;

  /// For dest group, this field saves the last key migrated by background
  /// pulling.
  bytes last_migrated_key = 7;

  /// The step of migration progress.
  MigrationStep step = 8;
}

/// EvalResult is the structured proposal payload.
message EvalResult {
  WriteBatchRep batch = 1;
  optional SyncOp op = 2;
}

/// WriteBatchRep is the serialized representation of DB write batch.
message WriteBatchRep { bytes data = 1; }

/// SyncOp is a structured message which contain operations must be executed in
/// order in all replicas.
message SyncOp {
  /// Add new shard to existing group.
  AddShard add_shard = 1;
  /// Purge an orphan replica.
  PurgeOrphanReplica purge_replica = 2;
  /// An event of shard migration.
  Migration migration = 3;

  /// A trick, force prost box the `SyncOp`, because `SyncOp` message is too
  /// large.
  EvalResult must_boxed = 128;
}

message AddShard { engula.server.v1.ShardDesc shard = 1; }

/// PurgeOrphanReplica is used by the replica leader. When the replica leader
/// finds an orphan replica, it can propose a command. After the command is
/// successfully executed, the replica can be shutdown safely.
message PurgeOrphanReplica { uint64 replica_id = 1; }

message Migration {
  enum Event {
    SETUP = 0;
    INGEST = 1;
    COMMIT = 2;
    ABORT = 3;
    /// Remove migration state.
    APPLY = 4;
  }

  Event event = 1;

  engula.server.v1.MigrationDesc migration_desc = 2;

  /// The latest migrated key, used for fault tolerance, locate the cursor that
  /// has been replicated.
  bytes last_ingested_key = 3;
}

message ReconcileTask {
  oneof task {
    CreateGroupTask create_group = 1;
    ReallocateReplicaTask reallocate_replica = 2;
    MigrateShardTask migrate_shard = 3;
    TransferGroupLeaderTask transfer_group_leader = 4;
    CreateCollectionShards create_collection_shards = 5;
  }
}

message CreateGroupTask {
  uint64 request_replica_cnt = 1;
  engula.server.v1.GroupDesc group_desc = 2;
  repeated engula.server.v1.NodeDesc wait_create = 3;
  repeated engula.server.v1.ReplicaDesc wait_cleanup = 4;
  CreateGroupTaskStep step = 5;
  uint64 create_retry = 6;
}

enum CreateGroupTaskStep {
  GROUP_INIT = 0;
  GROUP_CREATING = 1;
  GROUP_ROLLBACKING = 2;
  GROUP_FINISH = 3;
  GROUP_ABORT = 4;
}

message ReallocateReplicaTask {
  uint64 group = 1;
  uint64 src_node = 2;
  uint64 src_replica = 3;
  engula.server.v1.NodeDesc dest_node = 4;
  engula.server.v1.ReplicaDesc dest_replica = 5;
  ReallocateReplicaTaskStep step = 6;
}

enum ReallocateReplicaTaskStep {
  CREATING_DEST_REPLICA = 0;
  ADD_DEST_LEARNER = 1;
  REPLACE_DEST_VOTER = 2;
  SHED_SOURCE_LEADER = 3;
  REMOVE_SOURCE_MEMBERSHIP = 4;
  REMOVE_SOURCE_REPLICA = 5;
  REALLOCATE_FINISH = 6;
  REALLOCATE_ABORT = 7;
}

message MigrateShardTask {
  uint64 shard = 1;
  uint64 src_group = 2;
  uint64 dest_group = 3;
}

message TransferGroupLeaderTask {
  uint64 group = 1;
  uint64 target_replica = 5;
}

message CreateCollectionShards {
  repeated GroupShards wait_create = 1;
  repeated engula.server.v1.ShardDesc wait_cleanup = 2;
  CreateCollectionShardStep step = 3;
}

message GroupShards {
  uint64 group = 1;
  repeated engula.server.v1.ShardDesc shards = 2;
}

enum CreateCollectionShardStep {
  COLLECTION_CREATING = 0;
  COLLECTION_ROLLBACKING = 1;
  COLLECTION_FINISH = 2;
  COLLECTION_ABORT = 3;
}
